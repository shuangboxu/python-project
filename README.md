# python-project

## Overview
This repository hosts a team movie recommendation workflow that progresses from four independent scoring components to a merged recommendation list and static front-end deliverables. Every stage produces reproducible datasets, logs, and visual artefacts so that the full pipeline can be re-run from raw TMDB metadata.

## Key Components
- **Stage 1 – Component scoring scripts** (`scripts/01_*.py` to `scripts/04_*.py`)
  - `01_content_scores.py`: builds TF-IDF content features, engineered text metrics, and summary plots.
  - `02_rating_scores.py`: normalises vote averages, vote counts, and popularity with diagnostic charts.
  - `03_business_scores.py`: evaluates production companies and revenue proxies for business viability.
  - `04_release_geo.py`: analyses release timing, runtime suitability, and language coverage.
- **Stage 2 – Weighted merge** (`scripts/05_final_recommendations.py`)
  - Reads the four component CSVs, normalises each series, blends them with configurable weights, and exports both the full ranking and a Top-N slice.
- **Stage 3 – Presentation assets**
  - `reports/figures/`: ready-to-use PNGs generated by the scripts.
  - `reports/decks/`: placeholder for presentation decks.
  - `app/`: a static browser app that filters the final recommendations using `reports/logs/recommendation_data.json`.
  - `intro/`: a multilingual landing page for team introductions, including media assets.

## Repository Layout
```
python-project/
  app/                         # Static recommendation browser (index.html, app.js, styles.css)
  data/
    raw/                       # Source TMDB Excel exports (movies.xlsx, movies_10example.xlsx)
    processed/                 # Script-generated feature tables
    interim/                   # Reserved for manual intermediates
  intro/                       # Team introduction microsite with images and music
  notebooks/                   # Optional exploratory notebooks
  reports/
    decks/
    figures/                   # PNG outputs from Stage 1–4 scripts
    logs/                      # Run logs plus recommendation_data.json for the web app
    tables/                    # Component and merged CSV outputs
    visualizations/            # Placeholder for additional web artefacts
  scripts/                     # End-to-end scoring, QA, and utility scripts
  index.html                   # Root landing page routing to app/ or intro/
  requirements.txt
```

## Data Dependencies
- The primary input is `data/raw/movies.xlsx`. Keep a clean copy in `data/raw/`; do not edit in place.
- Component scripts create additional CSV exports (for example `data/processed/01_content_features.csv`). These files are version-controlled outputs for traceability.
- The Stage 3 app expects `reports/logs/recommendation_data.json`, which is generated after the final scoring merge.

## Environment Setup
1. Create and activate a Python 3.10+ virtual environment.
2. Install dependencies:
   ```bash
   pip install -r requirements.txt
   ```
3. (Optional) Install fonts listed in `NOTO_INSTALL.md` if running `04_release_geo.py` with custom font rendering.

## Running the Pipeline
1. Execute each component script from the project root to populate `reports/tables/` and `reports/figures/`:
   ```bash
   python scripts/01_content_scores.py
   python scripts/02_rating_scores.py
   python scripts/03_business_scores.py
   python scripts/04_release_geo.py
   ```
2. Merge the component scores and export final rankings:
   ```bash
   python scripts/05_final_recommendations.py --top_n 50
   ```
   Adjust `--top_n` as needed. The script writes `reports/tables/05_final_scores.csv`, `reports/tables/05_top_recommendations.csv`, and updates `reports/logs/05_final_merge_log.txt`.
3. Build the JSON payload for the web app:
   ```bash
   python scripts/generate_recommendation_dataset.py
   ```
   This command produces `reports/logs/recommendation_data.json`, which powers the client-side filters in `app/`.

## Viewing Stage 3 Assets
- **Static recommendation browser**: open `app/index.html` in a browser or serve the `app/` directory via a lightweight HTTP server (`python -m http.server 8000`).
- **Team microsite**: open `intro/about.html` or the language-specific pages in `intro/about/`.
- **Figures and decks**: the `reports/` directory holds all generated media for presentations.

## Testing and Quality Notes
- `test_placeholder.py` ensures the repository integrates with CI tooling; extend it as real tests are added.
- Logs in `reports/logs/` (for example `stage3_visualization_log.md`) should be updated whenever visual assets change.

---

## 项目概览
该仓库支持团队电影推荐工作流，从四个独立的评分组件出发，逐步合并生成最终推荐名单，并配套静态前端展示。每个阶段都会生成可复现的数据集、日志与可视化文件，便于使用 TMDB 原始元数据重新跑通全流程。

## 核心模块
- **第一阶段：组件评分脚本**（`scripts/01_*.py` 至 `scripts/04_*.py`）
  - `01_content_scores.py`：构建 TF-IDF 内容特征、文本工程指标与概览图表。
  - `02_rating_scores.py`：对评分均值、评分人数和热度进行归一化并输出诊断图。
  - `03_business_scores.py`：衡量制片公司、票房指标等商业可行性。
  - `04_release_geo.py`：分析上映时间、片长适配度以及语言覆盖情况。
- **第二阶段：加权合并**（`scripts/05_final_recommendations.py`）
  - 读取四个组件 CSV，标准化各列后按权重融合，导出完整排名与 Top-N 子集。
- **第三阶段：展示成果**
  - `reports/figures/`：脚本生成的 PNG 图像。
  - `reports/decks/`：演示文档占位目录。
  - `app/`：静态推荐浏览器，消费 `reports/logs/recommendation_data.json` 数据。
  - `intro/`：团队多语言介绍页面及配套多媒体资源。

## 仓库结构
```
python-project/
  app/                         # 静态网页（index.html、app.js、styles.css）
  data/
    raw/                       # 原始 TMDB Excel 数据
    processed/                 # 脚本生成的特征表
    interim/                   # 预留的中间数据
  intro/                       # 团队介绍站点与图片、音频
  notebooks/                   # 探索性 Notebook
  reports/
    decks/
    figures/                   # 阶段一至阶段四的图像输出
    logs/                      # 运行日志与 recommendation_data.json
    tables/                    # 各组件及合并后的 CSV
    visualizations/            # 预留的 Web 可视化目录
  scripts/                     # 全流程脚本与辅助工具
  index.html                   # 项目根目录入口页面
  requirements.txt
```

## 数据依赖
- 核心输入文件为 `data/raw/movies.xlsx`，请保持原始文件只读备份。
- 组件脚本会在 `data/processed/` 生成额外的 CSV，以便追踪特征构造细节。
- Stage 3 前端依赖 `reports/logs/recommendation_data.json`，由最终融合脚本运行后生成。

## 环境配置
1. 创建并激活 Python 3.10 以上的虚拟环境。
2. 安装依赖：
   ```bash
   pip install -r requirements.txt
   ```
3. （可选）若需在 `04_release_geo.py` 中使用指定字体，请参考 `NOTO_INSTALL.md` 安装字体。

## 跑通流程
1. 在项目根目录依次执行组件脚本，生成 `reports/tables/` 与 `reports/figures/`：
   ```bash
   python scripts/01_content_scores.py
   python scripts/02_rating_scores.py
   python scripts/03_business_scores.py
   python scripts/04_release_geo.py
   ```
2. 合并组件得分并导出最终排名：
   ```bash
   python scripts/05_final_recommendations.py --top_n 50
   ```
   可根据需求调整 `--top_n`。脚本会写入 `reports/tables/05_final_scores.csv`、`reports/tables/05_top_recommendations.csv`，并更新 `reports/logs/05_final_merge_log.txt`。
3. 为 Web 前端生成 JSON 数据：
   ```bash
   python scripts/generate_recommendation_dataset.py
   ```
   该命令会输出 `reports/logs/recommendation_data.json`，供 `app/` 下的前端页面读取。

## 查看第三阶段成果
- **静态推荐浏览器**：直接打开 `app/index.html`，或使用轻量服务器（如 `python -m http.server 8000`）进行本地预览。
- **团队介绍站点**：访问 `intro/about.html` 或 `intro/about/` 目录中的多语言页面。
- **图表与幻灯片**：相关资源存放在 `reports/` 子目录中，便于演示与复盘。

## 测试与质量说明
- `test_placeholder.py` 用于保持与持续集成的兼容，后续可在此扩展实际测试用例。
- 如更新展示资源，请同步维护 `reports/logs/` 内的运行日志（例如 `stage3_visualization_log.md`），记录生成方式与依赖。
